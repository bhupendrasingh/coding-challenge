# The following program takes internet domain as input and prints out the
# list of all email addresses found on that site.

# External dependency:
# Libraries used are the BeautifulSoup4 and Splinter for emulating the browser
# for scrapping the dynamic content generated by javascript.

import re
import sys
import urllib
import html.parser
import urllib.request
from bs4 import BeautifulSoup
from urllib.parse import urlsplit
from html.parser import HTMLParser
from splinter import Browser

all_email_ids=[] #stores all the email ids inlcuding duplicates.
lastlist=[]      # interim list for encoding the email ids text.
completeUrl=[]   # for tranforming the url from internal links(#)to absoulte url
unqiue_email_id_list=[]  # all unqiue email ids on the webpage

# main function that takes the internet domain as input and opens the url with
# browser object. The html content for the webpage is stored as text.The result
# is passed to the BeautifulSoup for html parsing and fetching all the links as
# well as email ids on the webpage.
def main(url):
    base_url=url
    domain_base_url=urlsplit(base_url)
    link_to_visit=[url]
    is_visited=[]
    while link_to_visit!=[] and link_to_visit[0] not in is_visited:
        url=link_to_visit.pop(0)
        is_visited.append(url)
        new_domain=urlsplit(url)
        if domain_base_url.netloc in new_domain.netloc:
            with Browser(('firefox')) as browser:
                browser.visit(url)
                res=browser.html
                soup=BeautifulSoup(res,'html.parser')
                email_extractor(soup)
                for link in soup.find_all('a'):
                    item=link.get('href',None)
                    completeUrl.append(urllib.parse.urljoin(url,item))
                link_to_visit=completeUrl
                for item in link_to_visit:
                    if item == base_url:
                        link_to_visit.remove(base_url)

# the function email_extrcator take the soup object a argument to navigate on
# the returned document text to find all the email ids on webpage.
def email_extractor(soup):
    all_email_ids=soup.select('a[href^=mailto]')
    for item in all_email_ids:
        if item.string!=None:
            lastlist.append(item.string.encode('utf-8').strip())
    unqiue_email_id_list=set(lastlist)
    for item in unqiue_email_id_list:
        print("Email IDs found so far!! :   ",item)

if __name__ == '__main__':
  main(sys.argv[1])
